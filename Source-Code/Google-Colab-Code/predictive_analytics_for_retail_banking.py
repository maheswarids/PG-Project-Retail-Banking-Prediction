# -*- coding: utf-8 -*-
"""Predictive Analytics for Retail Banking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hhg-ARpGUBx_144agXY6Lj8Qbgndy8Nc

# **Importing Libraries**
"""

import pandas as pd
import numpy as np
import math
from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc
from scipy.stats import ttest_ind, chi2_contingency
import pickle
import psycopg2

"""# **Loading Dataset**"""

df = pd.read_csv("bank.csv")

"""# **Basic Data Exploration**"""

#Printing the first five rows of the dataset
df.head()


#Information about the Dataset
print("Initial Data Info:")
print(df.info())


#Statistical Summary of the Dataset
print("\nStatistical Summary:")
print(df.describe())



#Checking Duplicate Rows
print(f"Duplicate Rows: {df.duplicated().sum()}")


#Check Unique Values in Categorical Columns
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    print(f"Unique values in {col}: {df[col].unique()}")

"""# **Exploratory Data Analysis**

1.Univariant Analysis(Analyzing individual features - Numerical & Categorial)

Histograms for Numerical Features
"""

# Select numerical features
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns

# Plot histograms for all numerical features
df[numerical_features].hist(figsize=(12, 8), bins=20, color='skyblue', edgecolor='black')
plt.suptitle("Distribution of Numerical Features")
plt.show()

"""Bar Plot for Categorical Features"""

plt.figure(figsize=(15, 8))

categorical_cols = df.select_dtypes(include=['object']).columns
num_cols = len(categorical_cols)  # Total categorical features

# Determine the number of rows and columns dynamically
rows = (num_cols // 3) + (num_cols % 3 > 0)  # Ensures all subplots fit

for i, col in enumerate(categorical_cols, 1):
    plt.subplot(rows, 3, i)
    sns.countplot(x=df[col], hue=df[col], palette='coolwarm', legend=False)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""2.Bivariate Analysis(Analyzing relationships between two features)

Boxplots to Identify Outliers
"""

plt.figure(figsize=(12, 6))
for i, col in enumerate(numerical_features):
    plt.subplot(2, 4, i+1)  # Adjust based on number of numerical features
    sns.boxplot(y=df[col], color='lightblue')
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()

"""Correlation Heatmap"""

from sklearn.preprocessing import LabelEncoder

df_encoded = df.copy()
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df_encoded[col])

# Plot heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Feature Correlation Heatmap")
plt.show()

"""3.Multivariate Analysis(Analyzing relationships among multiple variables together)

Pair Plot
"""

sns.pairplot(df, diag_kind='kde', corner=True)
plt.show()

"""Target Variable Analysis(Checking how the target variable ('y') is distributed)


"""

plt.figure(figsize=(6, 4))
sns.countplot(x=df['deposit'], hue=df['deposit'], palette='coolwarm', legend=False)
plt.title("Distribution of Target Variable")
plt.xlabel("Target Variable")
plt.ylabel("Count")
plt.show()

"""4.Hypothesis Testing

T-Test
"""

from scipy.stats import ttest_ind
import seaborn as sns
import matplotlib.pyplot as plt

# Split balance into two groups
balance_accepted = df[df['deposit'] == 'yes']['balance']
balance_rejected = df[df['deposit'] == 'no']['balance']

# Perform Independent T-Test
t_stat, p_value = ttest_ind(balance_accepted, balance_rejected, equal_var=False)

print(f"T-Test Statistic: {t_stat:.4f}")
print(f"P-Value: {p_value:.4f}")

# Interpretation
if p_value < 0.05:
    print("Reject the Null Hypothesis: Balance significantly affects deposit acceptance.")
else:
    print("Fail to Reject the Null Hypothesis: No significant difference in balance between groups.")

# Create box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x="deposit", y="balance", hue="deposit", data=df, palette=['red', 'green'], legend=False)

# Labels & Title
plt.xlabel("Deposit Accepted (yes,no)", fontsize=12)
plt.ylabel("Balance", fontsize=12)
plt.title("Balance Distribution for Accepted vs. Rejected Deposits", fontsize=14)

# Show plot
plt.show()

"""Chi-Square Test"""

from scipy.stats import chi2_contingency
import seaborn as sns
import matplotlib.pyplot as plt

# Identify categorical features dynamically
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

# Store Chi-Square results
chi2_results = {}

# Perform Chi-Square test for each categorical variable
for feature in categorical_features:
    contingency_table = pd.crosstab(df[feature], df['deposit'])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    chi2_results[feature] = chi2  # Store Chi-Square statistic

# Convert to DataFrame for visualization
chi2_df = pd.DataFrame(list(chi2_results.items()), columns=['Feature', 'Chi-Square Statistic'])
chi2_df = chi2_df.sort_values(by='Chi-Square Statistic', ascending=False)

# Plot the results
plt.figure(figsize=(10, 6))
sns.barplot(x='Chi-Square Statistic', y='Feature', hue='Feature', data=chi2_df, dodge=False, legend=False, palette='coolwarm')
plt.xlabel("Chi-Square Statistic", fontsize=14)
plt.ylabel("Categorical Feature", fontsize=14)
plt.title("Chi-Square Test Results for Categorical Features", fontsize=16)
plt.show()

"""# **Data Preprocessing**

Handling Missing Values
"""

print("\nMissing Values:")
print(df.isnull().sum())

missing_values = df.isnull().sum()
if missing_values.sum() == 0:
    print("No missing values found in the dataset.")
else:
    df.dropna(inplace=True)  # Only drop if missing values exist
    print("Missing values handled.")

"""Encode Categorical Variables"""

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("\nCategorical Columns:", categorical_cols)

print("\nCategorical Variables Before Encoding:")
print(df.head())

# Applying Label Encoding to categorical variables
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])  # Transforming categories into numerical values
    label_encoders[col] = le  # Storing encoders for later use

print("\nCategorical Variables After Encoding:")
print(df.head())  # Checking the transformation

"""**Saving the encoded model**"""

import pickle

# Save the LabelEncoders dictionary
with open("label_encoders.pkl", "wb") as f:
    pickle.dump(label_encoders, f)

print("LabelEncoders saved successfully!")

"""**Download the File**"""

from google.colab import files
files.download("label_encoders.pkl")

"""Handling Outliers"""

# Using IQR (Interquartile Range) to remove outliers
def remove_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]


# Apply outlier removal (if necessary) to numerical features
for col in numerical_features:
    df = remove_outliers(df, col)



# Get the number of numerical features
num_features = len(numerical_features)

# Define grid size dynamically
rows = math.ceil(num_features / 4)  # Maximum 4 plots per row

# ---- Visualizing Outliers After Handling ----
plt.figure(figsize=(15, rows * 4))  # Adjust figure size dynamically

for i, col in enumerate(numerical_features, 1):  # Start index from 1
    plt.subplot(rows, 4, i)  # Dynamic row allocation
    sns.boxplot(y=df[col], color='lightgreen')  # Different color for after handling
    plt.title(f"Boxplot of {col} (After Handling)")

plt.tight_layout()
plt.show()

"""Feature Scaling (Standardization)"""

#Separate features (X) and target (y)
X = df.drop(columns=['deposit'])  # Drop target variable before scaling
y = df['deposit']  # Keep y as it is

# Apply scaling only to features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Scale only feature columns

print("\nFeature Scaling Applied (Standardization).")

"""# **Model Training**

Splitting the Dataset
"""

# Define features (X) and target variable (y)
X = df.drop(columns=['deposit'])  # Features
y = df['deposit']  # Target variable

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardizing the feature values for models that require scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data
X_test_scaled = scaler.transform(X_test)  # Transform test data (using same scaler)

print("Data Splitting Completed: 80% Train - 20% Test")

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, confusion_matrix, classification_report, roc_curve
import matplotlib.pyplot as plt

# Initialize Logistic Regression
lr = LogisticRegression(max_iter=500)
lr.fit(X_train_scaled, y_train)

# Cross-validation accuracy
lr_acc = cross_val_score(lr, X_train_scaled, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("\n Logistic Regression Cross-validation Accuracy:", lr_acc)

# Predict on test data
y_pred_lr = lr.predict(X_test_scaled)
y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]  # Probabilities for ROC curve

# Accuracy, Recall, and ROC Score
acc_lr = accuracy_score(y_test, y_pred_lr) * 100
rec_lr = recall_score(y_test, y_pred_lr) * 100
roc_lr = roc_auc_score(y_test, y_proba_lr) * 100
print(f"\n Logistic Regression Accuracy: {acc_lr:.2f}% | Recall: {rec_lr:.2f}% | ROC AUC: {roc_lr:.2f}%")

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_lr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, linewidth=2, label=f"AUC: {roc_lr:.2f}", color='b')
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve: Logistic Regression', fontsize=14)
plt.legend()
plt.show()

"""**Random Forest**"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve

# Initialize Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Cross-validation accuracy
rf_acc = cross_val_score(rf, X_train_scaled, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("\n Random Forest Cross-validation Accuracy:", rf_acc)

# Predict on test data
y_pred_rf = rf.predict(X_test_scaled)
y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]

# Accuracy, Recall, and ROC Score
acc_rf = accuracy_score(y_test, y_pred_rf) * 100
rec_rf = recall_score(y_test, y_pred_rf) * 100
roc_rf = roc_auc_score(y_test, y_proba_rf) * 100
print(f"\n Random Forest Accuracy: {acc_rf:.2f}% | Recall: {rec_rf:.2f}% | ROC AUC: {roc_rf:.2f}%")

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_rf)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, linewidth=2, label=f"AUC: {roc_rf:.2f}", color='b')
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve: Random Forest', fontsize=14)
plt.legend()
plt.show()

"""**Support Vector Machine**"""

from sklearn.svm import SVC

# Initialize SVM with probability=True for ROC Curve
svm = SVC(probability=True)
svm.fit(X_train_scaled, y_train)

# Cross-validation accuracy
svm_acc = cross_val_score(svm, X_train_scaled, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("\n SVM Cross-validation Accuracy:", svm_acc)

# Predict on test data
y_pred_svm = svm.predict(X_test_scaled)
y_proba_svm = svm.predict_proba(X_test_scaled)[:, 1]

# Accuracy, Recall, and ROC Score
acc_svm = accuracy_score(y_test, y_pred_svm) * 100
rec_svm = recall_score(y_test, y_pred_svm) * 100
roc_svm = roc_auc_score(y_test, y_proba_svm) * 100
print(f"\n SVM Accuracy: {acc_svm:.2f}% | Recall: {rec_svm:.2f}% | ROC AUC: {roc_svm:.2f}%")

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_svm)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, linewidth=2, label=f"AUC: {roc_svm:.2f}", color='b')
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve: SVM', fontsize=14)
plt.legend()
plt.show()

"""**XGBClassifier**"""

from xgboost import XGBClassifier

# Initialize XGBoost
xgb = XGBClassifier(eval_metric='logloss', random_state=42)
xgb.fit(X_train_scaled, y_train)

# Cross-validation accuracy
xgb_acc = cross_val_score(xgb, X_train_scaled, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("\n XGBoost Cross-validation Accuracy:", xgb_acc)

# Predict on test data
y_pred_xgb = xgb.predict(X_test_scaled)
y_proba_xgb = xgb.predict_proba(X_test_scaled)[:, 1]

# Accuracy, Recall, and ROC Score
acc_xgb = accuracy_score(y_test, y_pred_xgb) * 100
rec_xgb = recall_score(y_test, y_pred_xgb) * 100
roc_xgb = roc_auc_score(y_test, y_proba_xgb) * 100
print(f"\n XGBoost Accuracy: {acc_xgb:.2f}% | Recall: {rec_xgb:.2f}% | ROC AUC: {roc_xgb:.2f}%")

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba_xgb)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, linewidth=2, label=f"AUC: {roc_xgb:.2f}", color='b')
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve: XGBoost', fontsize=14)
plt.legend()
plt.show()

"""**KNeighbour Classifier**"""

from sklearn.neighbors import KNeighborsClassifier

# Initialize KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

# Cross-validation accuracy
knn_acc = cross_val_score(knn, X_train_scaled, y_train, cv=3, scoring='accuracy', n_jobs=-1)
print("\n KNN Cross-validation Accuracy:", knn_acc)

# Predict on test data
y_pred_knn = knn.predict(X_test_scaled)

# Accuracy
acc_knn = accuracy_score(y_test, y_pred_knn) * 100
print(f"\n KNN Accuracy: {acc_knn:.2f}%")

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))

"""**Prints the best model**"""

# Store accuracy scores in a dictionary
model_results = {
    "Logistic Regression": acc_lr,
    "Random Forest": acc_rf,
    "SVM": acc_svm,
    "XGBoost": acc_xgb,
    "KNN": acc_knn
}

# Find the best model
best_model = max(model_results, key=model_results.get)

# Print the best model
print(f"\n Best Model: {best_model} with Accuracy: {model_results[best_model]:.2f}%")

"""**Save the Best Model**"""

# Save the trained Random Forest model to a binary file
with open('random_forest_model.pkl', 'wb') as f:
    pickle.dump(rf, f)

print(type(rf))

"""**Converting the model file to binary**"""

def convert_model_to_binary(file_path):
    with open(file_path, "rb") as file:
        binary_data = file.read()
    return binary_data

# Convert the saved model file to binary
binary_model = convert_model_to_binary("random_forest_model.pkl")

"""**Downloading the File**"""

from google.colab import files

files.download("random_forest_model.pkl")

"""**Plotting ROC Curves**"""

def plot_roc_curves(models, X_test, y_test):
    plt.figure(figsize=(10, 6))

    for name, model in models.items():
        # Convert X_test to NumPy array to avoid feature names issue
        X_test_np = X_test.values

        # Get predicted probabilities for positive class (1)
        y_probs = model.predict_proba(X_test_np)[:, 1]

        # Compute ROC curve and AUC score
        fpr, tpr, _ = roc_curve(y_test, y_probs)
        roc_auc = auc(fpr, tpr)

        # Plot each model's ROC curve
        plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

    # Plot baseline (random guessing)
    plt.plot([0, 1], [0, 1], 'k--', label="Random Guessing (AUC = 0.50)")

    # Graph Labels
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve Comparison")
    plt.legend(loc="lower right")
    plt.show()

# Dictionary of trained models
trained_models = {
    "Logistic Regression": lr,
    "Random Forest": rf,
    "SVM": svm,
    "XGBoost": xgb,
    "KNN": knn
}

# Plot ROC curves for all models
plot_roc_curves(trained_models, X_test, y_test)

"""**Hyperparameter Tuning**

**Random Forest**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.metrics import accuracy_score

# Define the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Optimized parameter distribution (with corrected 'max_features')
param_dist = {
    'n_estimators': randint(100, 200),
    'max_depth': randint(10, 30),
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2'],  # Corrected max_features
    'bootstrap': [True, False]
}

# Apply RandomizedSearchCV with 20 iterations and 3-fold cross-validation
random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,
                                      n_iter=20, cv=3, n_jobs=-1, verbose=2, random_state=42)

# Fit the model
random_search_rf.fit(X_train, y_train)

# Best hyperparameters found
print("Best Parameters for Random Forest:", random_search_rf.best_params_)

# Get the best model
best_rf = random_search_rf.best_estimator_

# Predict using the best Random Forest model
y_pred_rf = best_rf.predict(X_test)

# Calculate accuracy of the best Random Forest model
rf_accuracy = accuracy_score(y_test, y_pred_rf) * 100
print(f"Random Forest Model Accuracy: {rf_accuracy:.2f}")